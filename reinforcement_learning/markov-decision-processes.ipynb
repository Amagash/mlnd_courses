{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "Definition:\n",
    "- Single agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The World\n",
    "\n",
    "Let's define the physics of the world :\n",
    "\n",
    "It's a 3 by 4 grid. The agent's possible actions are UP, DOWN, RIGHT, LEFT. When hitting the border of the map, the agent stays where he is. The black square acts like a wall and can't be walked through. The red square must be avoided and ends the game if walk on. The green square is the goal to achieve and ends the game when walked on.\n",
    "\n",
    "![alt text](world.png \"world\")\n",
    "\n",
    "**Quiz** : Given this particular world, the shortest sequence of actions that could lead to the green square are:\n",
    "- UP, UP, RIGHT, RIGHT, RIGHT\n",
    "- RIGHT, RIGHT, UP, UP, RIGHT\n",
    "\n",
    "**Quiz** : Now let's change the world by addind some uncertainty, some stochasticity to it. When executing an action, it's executed with a probability of 0.8 and 0.2 of the time the agent move perpendicularly. What is the probabilty of the sequence UP, UP, RIGHT, RIGHT, RIGHT to actually succeed ?\n",
    "\n",
    "$$\n",
    "P = 0.8^5 + 0.1^4 \\times 0.8 = 0.32776\n",
    "$$\n",
    "\n",
    "Because we first count the probability for the sequence to always work as expected, meaning falling into the 0.8 probability of having the right action being executed PLUS the 0.1 probability of not going in the expected direction 4 times and the final right leading to the goal square.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Markov decision process framework\n",
    "\n",
    "Markovian property : \n",
    "- Only the present matters. This means that the transition function only depends on the current state s.\n",
    "- The world, the  model, the physics are stationary. It means that the rules don't change over time.\n",
    "\n",
    "The Markov Decision Process or MDP is defined by States, a Model, Actions and Rewards. Those four thing define a problem. The Policy defines a solution.\n",
    "\n",
    "Name | Definition | Meaning\n",
    ":---: | :---: | :---:\n",
    "STATES | $$S$$ | There are a set of tokens that represents every state that the agent could be in. In the previous example it would be the position (x, y coordinate) in the grid.\n",
    "MODEL | $$T (s, a, s') \\to P (s'\\;|\\; s, a)$$ | The rules of the game or physics of the world. Sometimes refered as the transition model or transition function. $T (s, a, s')$ is a function of three variables (s = state, a = action, s' = another state). This function produces the probability $P (s'| s, a)$ that you will end up transitioning to state s' given that you were in state s and took action a.\n",
    "ACTIONS | $$A(s), A$$ | Actions are the things you can do in a particular state. (ex : UP, DOWN, RIGHT, LEFT)\n",
    "REWARD | $$R(s), R(s, a), R(s, a, s')$$ | A scalar value that you get from being in a state. It basically tells you the usefulness of being in that state. It can be defined differently, $R(s)$ is the rewards for being in state s, $R(s, a)$ is the reward for being in a state s and taking an action a, $R(s, a, s')$ is the reward for being in a state s and taking an action a and ending up in a state s'.\n",
    "---|---|---\n",
    "POLICY | $$(\\pi(s) \\to a) \\quad\\pi^*$$ | A function that takes in a state s and returns an action a. In other words, for any given state that you're in, it tells you the action you should take. $\\pi^*$ is the optimal policy. It is tha policy that maximizes your long-term expected reward. For example $\\pi(START) = UP $\n",
    "\n",
    "## Formalizing the MDP Model\n",
    "\n",
    "$$MDP = \\{S, A, T, R, \\gamma\\}$$\n",
    "In the Pacman game :\n",
    "1. Pacman states S are $\\{$ all position of pacman, ghosts, food and pellets $\\}$\n",
    "2. Pacman actions A are $\\{$ N, S, E, W $\\}$\n",
    "3. Pacman model T (transition model) are $\\{$ move directions, die from ghosts, eat food, ... $\\}$\n",
    "4. Pacman rewards R are $\\{$ -1 per steps, +10 food, -500 die, +500 win, ... $\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about rewards\n",
    "\n",
    "The temporal credit assignment problem :\n",
    "Of all the actions you take, what was the action that led you to the reward you get ?\n",
    "\n",
    "If we set that for each step taken in the world $R(s) = -0.04$, except for the red square $R(s) = -1$ and for the green square $R(s) = +1$. Given that, what would be the best policy (the best set of actions to take)?\n",
    "\n",
    "The best policy is to go up when starting from the bottom left. When starting on the bottom right, it is best to go all the way around to avoid the 0.1 probality of chance to end up in the -1 square. This shows that minor changes to the reward function actually matters a lot.\n",
    "\n",
    "![alt text](policy.png \"world\")\n",
    "\n",
    "Quiz : What are the best policies considering the reward functions ?\n",
    "For the first one, we see that the reward for each step is bigger than the reward to end the game, therefore the best solution for best solution for the agent is to take the longest path. Whereas for the second one, the negative reward strongly encourages to end the game even though it means loosing (going in the red square)\n",
    "\n",
    "![alt text](reward-quiz.png \"world\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence of rewards\n",
    "\n",
    "**Infinite horizons** : The policy also depends on the time the agent has. Having an infinite amount of time would probably give a different policy than having a finite amount of time. So the policy $\\pi (s, t) \\to a$ is also a function of both the statut s you're in and the time step t you're in and that might lead to a different set of actions a.\n",
    "\n",
    "**Utility of sequences** : Utilities of sequences means we have some function $U(s_0, s_1, s_2, \\dots)$ over the sequence of state that we're going to see.\n",
    "\n",
    "if $U(s_0, s_1, s_2, \\dots) > U(s_0, s_1', s_2', \\dots)$\n",
    "\n",
    "then $U(s_1, s_2, \\dots) > U(s_1', s_2', \\dots)$\n",
    "\n",
    "This is called stationay preferences, it means if I prefer one sequence of state today over another sequence of states, then I prefer that sequence of states over the same sequence of states tomorrow.\n",
    "\n",
    "Mathematically we write this as follows:\n",
    "\n",
    "$$U(s_0, s_1, s_2, \\dots) = \\sum_{t=o}^{\\infty} R(s_t)$$\n",
    "\n",
    "The utility $U$ that we receive for visiting a sequence of states $s_0, s_1, s_2, \\dots$ is the sum of all the rewards $R(s_t)$ that we will receive for visiting those states.\n",
    "\n",
    "Quiz : Considering all numbers are rewards, would you rather be on the top side or bottom side ?\n",
    "![](sequence-state.png \"world\")\n",
    "The answer one is neither because both utilities of sequences are equal to infinity. This can be solved by adding something to the equation above which is called **discounted rewards**:\n",
    "\n",
    "$$U(s_0, s_1, s_2, \\dots) = \\sum_{t=o}^{\\infty} \\gamma^t R(s_t)\\qquad 0\\leqslant\\gamma<1$$ \n",
    "\n",
    "By multiplying the reward $R(s_t)$ for a certain state at time t with $\\gamma^t$, we essentially decrease the reward over time because $0\\leqslant\\gamma<1$. For a number comprised between 1 and 0, increasing the power $t$ will bring $\\gamma^t$ to zero. Note that $\\gamma$ can never be equal to 1 because then we would go back to the previous expression where everything add up and goes to infinity.\n",
    "The expression is always bounded above by the following geometric series:\n",
    "$$\\sum_{t=o}^{\\infty} \\gamma^t R_{max} = \\frac{R_{max}}{1-\\gamma}$$\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "$$\\sum_{t=0}^{\\infty} \\gamma^t R_{max}$$\n",
    "$$(\\sum_{t=0}^{\\infty} \\gamma^t) R_{max}$$\n",
    "Let's define \n",
    "$$x = \\sum_{t=0}^{\\infty} \\gamma^t$$\n",
    "$$x = (\\gamma^0 + \\gamma^1 + \\gamma^2 + \\dots)$$\n",
    "$$x = (1 + \\gamma^1 + \\gamma^2 + \\dots)$$\n",
    "$$x = 1 + (\\gamma^1 + \\gamma^2 + \\dots)$$\n",
    "$$x = 1 + \\gamma(\\gamma^0 + \\gamma^1 + \\dots)$$\n",
    "$$x = 1 + \\gamma x$$\n",
    "$$x - \\gamma x = 1$$\n",
    "$$x (1 - \\gamma)= 1$$\n",
    "$$x = \\frac{1}{(1 - \\gamma)}$$\n",
    "Therefore\n",
    "$$\\sum_{t=o}^{\\infty} \\gamma^t R_{max} = \\frac{R_{max}}{1-\\gamma}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maths behind POLICY\n",
    "\n",
    "The optimal policy $\\pi^*$ is the one that maximizes $argmax$ our long-term expected reward. We have an expected value $E$ of the sum $\\sum_t$ of the discounted reward $\\gamma^tR(s_t)$ at time $t$ given $\\pi$. So I would like to know the policy that maximizes the value of that expression, so it gives us the highest expected reward.\n",
    "$$\\pi^* = argmax_{\\pi} \\;  E\\,[\\sum_{t=0}^{\\infty}\\;\\gamma^t\\,R(s_t)\\;|\\;\\pi]$$\n",
    "\n",
    "The utility of a particular state is going to depend upon the policy we're following $U^{\\pi}$ and that is going to be the expected $E$ set of states that I'm going to see from that point on given that I've followed the policy. How good is it to be in some state ? It's exactly as good to be in that state as what we will expect to see from that point on, given that we're following a specific policy $\\pi$ where we started in that state $s_0=s$. Note, the reward for entering a state is not the same thing as the utility for that state $R(s)\\ne U^{\\pi}(s) $. What reward gives us is immediate feedback, but utility gives us long term feedback. Utility stateis both the reward we get for that state but also all the rewards that we're going to get from that point on. Utilities are really about accounting for all the delayed rewards.\n",
    "$$U^{\\pi}(s) = E\\,[\\sum_{t=0}^{\\infty}\\;\\gamma^t\\,R(s_t)\\;|\\;\\pi, \\quad s_0=s]$$\n",
    "\n",
    "The optimal policy is the one that every state returns the action that maximizes my expected utility.\n",
    "$$\\pi^* (s) = argmax_{a} \\sum_{s'}\\; T(s, a, s') \\; U^{\\pi^*}(s') \\qquad knowing\\; that\\; T (s, a, s') \\to P (s'\\;|\\; s, a)$$\n",
    "\n",
    "The true utility being in a state $U^{\\pi^*}(s)$ is the reward you get in that state $R(s)$ plus the discount $\\gamma$ of all the rewards you're going to get at that point which is defined as the utility you're going to get  for the states that you see $U^{\\pi^*}(s')$.\n",
    "The **Bellman equation** :\n",
    "$$U^{\\pi^*}(s) = R(s) + \\gamma\\; max_{a} \\sum_{s'}\\; T(s, a, s') \\; U^{\\pi^*}(s')$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Solving the Bellman Equation\n",
    "\n",
    "$$U^{\\pi^*}(s) = R(s) + \\gamma\\; max_{a} \\sum_{s'}\\; T(s, a, s') \\; U^{\\pi^*}(s')$$\n",
    "\n",
    "Utility of s $U^{\\pi^*}(s)$\n",
    "Let say we have n states, which means we have n equations in n unknowns which we know how to solve if the equations are linear. However, those equations are ot linear because of the $max$. So here is the algorithm we use that works to solve this:\n",
    "1. start with arbitrary utilities\n",
    "2. updates utilities based on their neighbors. Based on the states, you're going to update the utility for a state based on all of the states tha it can reach.\n",
    "3. repeat until convergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding policies\n",
    "\n",
    "Starting utilities at time 0 is 0 except at the absorbing states 1 and -1.\n",
    "How is the utility going to evolve in the square marked with a little cross after one $U_1(s)$ and two $U_2(s)$ iterations.\n",
    "\n",
    "![](find-policies.png)\n",
    "\n",
    "Calculation at iteration 1:\n",
    "\n",
    "$$U_1^x(s) = R(s) + \\gamma\\; max_{a} \\sum_{s'}\\; T(s, a, s') \\; U^{\\pi^*}(s')$$\n",
    "$$U_1^x(s) = -0.04 + 0.5\\; max_{a} [UP: 0.8\\times0+0.1\\times1+0.1\\times0 ; DOWN:0.8\\times0+0.1\\times1+0.1\\times0 ; LEFT : 0.8\\times0+0.1\\times0+0.1\\times0 ; RIGHT : 0.8\\times1+0.1\\times0+0.1\\times0]$$\n",
    "$$U_1^x(s) = -0.04 + 0.5\\; max_{a} [UP: 0.1 ; DOWN:0.1 ; LEFT : 0 ; RIGHT : 0.8]$$\n",
    "$$U_1^x(s) = -0.04 + 0.5 \\times 0.8$$\n",
    "$$U_1^x(s) = 0.36$$\n",
    "\n",
    "Let's figure out for the square down :\n",
    "$$U_1^d(s) = -0.04 + 0.5\\; max_{a} [UP: 0.8\\times0+0.1\\times-1+0.1\\times0 ; DOWN:0.8\\times0+0.1\\times-1+0.1\\times0 ; LEFT : 0.8\\times0+0.1\\times0+0.1\\times0 ; RIGHT : 0.8\\times-1+0.1\\times0+0.1\\times0]$$\n",
    "$$U_1^d(s) = -0.04 + 0.5\\; max_{a} [UP: -0.1 ; DOWN:-0.1 ; LEFT : 0 ; RIGHT : -0.8]$$\n",
    "$$U_1^d(s) = -0.04 + 0.5 \\times 0$$\n",
    "$$U_1^d(s) = -0.04$$\n",
    "\n",
    "Let's figure out for the square on the left :\n",
    "$$U_1^l(s) = -0.04 + 0.5\\; max_{a} [UP: 0.8\\times0+0.1\\times0+0.1\\times0 ; DOWN:0.8\\times0+0.1\\times0+0.1\\times0 ; LEFT : 0.8\\times0+0.1\\times0+0.1\\times0 ; RIGHT : 0.8\\times0+0.1\\times0+0.1\\times0]$$\n",
    "$$U_1^l(s) = -0.04 + 0.5\\; max_{a} [UP: 0 ; DOWN: 0 ; LEFT : 0 ; RIGHT : 0]$$\n",
    "$$U_1^l(s) = -0.04 + 0.5 \\times 0$$\n",
    "$$U_1^l(s) = -0.04$$\n",
    "\n",
    "Therefore for the next iteration we have:\n",
    "![](find-policies2.png)\n",
    "\n",
    "Calculation at iteration 2:\n",
    "\n",
    "$$U_2^x(s) = R(s) + \\gamma\\; max_{a} \\sum_{s'}\\; T(s, a, s') \\; U^{\\pi^*}(s')$$\n",
    "$$U_2^x(s) = -0.04 + 0.5\\; max_{a} [UP: 0.8\\times0.36+0.1\\times-0.04+0.1\\times1 ; DOWN:0.8\\times-0.04+0.1\\times1+0.1\\times-0.04 ; LEFT : 0.8\\times-0.04+0.1\\times0.36+0.1\\times-0.04 ; RIGHT : 0.8\\times1+0.1\\times0.36+0.1\\times-0.04]$$\n",
    "$$U_2^x(s) = -0.04 + 0.5\\; max_{a} [UP: 0.384 ; DOWN:0.064 ; LEFT : 0 ; RIGHT : 0.832]$$\n",
    "$$U_2^x(s) = -0.04 + 0.5 \\times 0.832$$\n",
    "$$U_2^x(s) = 0.376$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveman's world\n",
    "\n",
    "The caveman is described as follows:\n",
    "\n",
    "![](caveman.png)\n",
    "\n",
    "\n",
    "Quiz : What is the utility of being in the state Hungry ?\n",
    "\n",
    "$$U(s) = R(s) + \\gamma\\; \\sum_{s'}\\; P (s'\\;|\\; s) \\; U(s')$$\n",
    "$$U(H) = R_H + \\gamma\\; (P_{HH}\\times R_H + P_{HG}\\times R_G + P_{HF}\\times R_F + P_{HD}\\times R_D)$$\n",
    "$$U(H) = 0 + 0.9 \\;(0.5\\times 0 + 0.4\\times 1 + 0\\times 10 + 0.1\\times -10)$$\n",
    "$$U(H) = -0.54 $$\n",
    "\n",
    "And the other states ?\n",
    "$$U(G) = R_G + \\gamma\\; (P_{GH}\\times R_H + P_{GG}\\times R_G + P_{GF}\\times R_F + P_{GD}\\times R_D)$$\n",
    "$$U(G) = 1 + 0.9 \\;(0.2\\times 0 + 0.1\\times 1 + 0.6\\times 10 + 0.1\\times -10)$$\n",
    "$$U(G) = 5.59 $$\n",
    "\n",
    "$$U(F) = R_F + \\gamma\\; (P_{FH}\\times R_H + P_{FG}\\times R_G + P_{FF}\\times R_F + P_{FD}\\times R_D)$$\n",
    "$$U(F) = 10 + 0.9 \\;(0.9\\times 0 + 0\\times 1 + 0\\times 10 + 0.1\\times -10)$$\n",
    "$$U(F) = 9.1 $$\n",
    "\n",
    "$$U(D) = R_D + \\gamma\\; (P_{DH}\\times R_H + P_{DG}\\times R_G + P_{DF}\\times R_F + P_{DD}\\times R_D)$$\n",
    "$$U(D) = -10 + 0.9 \\;(0\\times 0 + 0\\times 1 + 0\\times 10 + 0.1\\times -10)$$\n",
    "$$U(D) = -19 $$\n",
    "\n",
    "For the next step :\n",
    "$$U(H) = 0 + 0.9 \\;(0.5\\times -0.54 + 0.4\\times 5.59 + 0\\times 9.1 + 0.1\\times -19)$$\n",
    "$$U(H) = 0.06 $$\n",
    "\n",
    "$$U(G) = 1 + 0.9 \\;(0.2\\times -0.54 + 0.1\\times 5.59 + 0.6\\times 9.1 + 0.1\\times -19)$$\n",
    "$$U(G) = 4.61 $$\n",
    "\n",
    "$$U(F) = 10 + 0.9 \\;(0.9\\times -0.54 + 0\\times 5.59 + 0\\times 9.1 + 0.1\\times -19)$$\n",
    "$$U(F) = 7.85 $$\n",
    "\n",
    "$$U(D) = -10 + 0.9 \\;(0\\times -0.54 + 0\\times 5.59 + 0\\times 9.1 + 0.1\\times -19)$$\n",
    "$$U(D) = -27.1 $$\n",
    "\n",
    "![](k-step.png)\n",
    "\n",
    "- Markov processes represent uncertainty in state transition\n",
    "- It is possible to determine the overall value of a state\n",
    "- What's next? Adding actions !\n",
    "\n",
    "## The value of free-will\n",
    "\n",
    "![](free-will.png)\n",
    "$$\n",
    "T(s, a, s') = P(s'\\;|\\;s, a)=\n",
    "\\begin{bmatrix}\n",
    "  P^a_{11} & P^a_{12} & \\dots & P^a_{1n} \\\\\n",
    "  P^a_{21} & P^a_{22} & \\dots & P^a_{2n} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "  P^a_{n1} & P^a_{n2} & \\dots & P^a_{nn} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Value iteration needs one more thing:\n",
    "$$U(s) = max_{a} [R(s, a) + \\gamma\\; \\sum_{s'}\\; P (s'\\;|\\; s, a) \\; U(s')]$$\n",
    "\n",
    "Quiz : What are the free-will values ?\n",
    "\n",
    "$$U(H) = max_{a} [R_H + \\gamma\\; ((P_{HH}^{sleep}\\times R_H + P_{HD}^{sleep}\\times R_D) \\;;\\; (P_{HG}^{hunt}\\times R_G + P_{HD}^{hunt}\\times R_D))]$$\n",
    "$$U(H) = max_{a} [0 + 0.9\\; ((0.7\\times 0 + 0.3\\times -10) \\;;\\; (0.9\\times 1 + 0.1\\times -10))]$$\n",
    "$$U(H) = max_{a} [0 + 0.9\\; (-3 \\;;\\; -0.1)]$$\n",
    "$$U(H) = max_{a} [-2.7 \\;;\\; -0.09]$$\n",
    "$$U(H) = -0.09 $$\n",
    "\n",
    "![](free-will-values.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
